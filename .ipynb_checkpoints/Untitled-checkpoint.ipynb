{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isdir\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def face_extract(filename,required_size=(160,160)):  \n",
    "    image = Image.open(filename)    \n",
    "    image = image.convert('RGB')                # convert to RGB, if needed    \n",
    "    image = np.asarray(image)   \n",
    "    detector = MTCNN()                          # create the detector, using default weights  \n",
    "    results = detector.detect_faces(image)     # detect faces in the image    \n",
    "    if len(results):\n",
    "        faces,boxes=[],[]\n",
    "        for i in range(len(results)):\n",
    "            x1, y1, width, height = results[i]['box']    # extract the bounding box from the first face    \n",
    "            x1, y1 = abs(x1), abs(y1)\n",
    "            x2, y2 = x1 + width, y1 + height\n",
    "            face = image[y1:y2, x1:x2]                  # extract the face\n",
    "            face=cv2.resize(face,required_size)\n",
    "            faces.append(face)\n",
    "            boxes.append([x1,y1,width,height])\n",
    "        return faces,boxes\n",
    "    else:\n",
    "        return ([],[])    \n",
    "# load images and extract faces for all images in a directory\n",
    "def load_faces(directory):\n",
    "    faces = list()  \n",
    "    for filename in listdir(directory):      \n",
    "        path = directory + filename\n",
    "        face = face_extract(path)                # get face\n",
    "        if face is None:\n",
    "            continue\n",
    "        faces.append(face)\n",
    "    return faces\n",
    "\n",
    "\n",
    "# load a dataset that contains one subdir for each class that in turn contains images\n",
    "def load_dataset(directory):\n",
    "    X, y = list(), list()\n",
    "    people_dict={}\n",
    "    for i,subdir in enumerate(listdir(directory)):\n",
    "        path = directory + subdir + '/'        \n",
    "        if not isdir(path):               # skip any files that might be in the dir\n",
    "            continue\n",
    "        faces = load_faces(path)          # load all faces in the subdirectory\n",
    "        people_dict.update({i:subdir})    # create key to people dictionary\n",
    "        print('loaded %d images for person: %s' % (len(faces), subdir))\n",
    "        labels=[subdir for _ in range(len(faces))]\n",
    "        X.extend(faces)\n",
    "        y.extend(labels)\n",
    "    return np.asarray(X), np.asarray(y),people_dict\n",
    "\n",
    "def face_to_embedding(face,model):\n",
    "        face = face.astype('float32')      \n",
    "        mean, std = face.mean(), face.std()\n",
    "        face_norm = (face - mean) / std        # normalize face pixels\n",
    "        face_norm = np.expand_dims(face_norm, axis=0)       \n",
    "        return model.predict(face_norm)        # make prediction to get embedding and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "#load the model and people dictionary\n",
    "FR_model=joblib.load('model/FR_model.pkl')\n",
    "embed_model=load_model('facenet_model/facenet_keras.h5')\n",
    "people_dict=joblib.load('database/people_dictionary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is no face in the image\n"
     ]
    }
   ],
   "source": [
    "image_path='C:\\\\Users\\\\Admin\\\\My_Projects\\\\facenet-tf2\\\\images\\\\k.png'\n",
    "def find_people(img_path,model,embed_model):\n",
    "    faces,boxes=face_extract(img_path)\n",
    "    if len(faces):\n",
    "        img=cv2.imread(image_path,1)\n",
    "        for i in range(len(faces)):\n",
    "            face,(x,y,w,h)=faces[i],boxes[i]\n",
    "            embed=face_to_embedding(face,embed_model)\n",
    "            Nembed=Normalizer(norm='l2').transform(embed)\n",
    "            pred=model.predict_proba(Nembed)\n",
    "            prob=pred[0][np.argmax(pred[0])]\n",
    "            label=people_dict[np.argmax(pred[0])]+(\" %.2f\"%(prob*100)) if prob >.65 else 'unknown'                      \n",
    "            cv2.rectangle(img,(x,y), (x+w,y+h), (255,255,0), 1)\n",
    "            cv2.putText(img,label, (int(x), int(y-15)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,0), 2)\n",
    "        try:\n",
    "\n",
    "            cv2.imshow('image',img)\n",
    "            if cv2.waitKey(0)==ord('q'):\n",
    "         #       cv2.imwrite('test7.jpg',img)\n",
    "                cv2.destroyAllWindows() \n",
    "        finally:\n",
    "            cv2.destroyAllWindows()\n",
    "    else:\n",
    "        print('there is no face in the image')\n",
    "\n",
    "find_people(image_path,FR_model,embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_frontalface_default.xml') #extract face feature\n",
    "#cap=cv2.VideoCapture('http://192.168.1.100:4747/video')\n",
    "cap=cv2.VideoCapture(0)\n",
    "#fourcc=cv2.VideoWriter_fourcc(*'XVID')\n",
    "#out=cv2.VideoWriter('recorded_frame.avi',fourcc,2,(640,480))\n",
    "try: \n",
    "    while True:\n",
    "        _,frame=cap.read()\n",
    "        faces = face_cascade.detectMultiScale(frame, 1.3, 5)\n",
    "        if len(faces):\n",
    "            for face in faces:\n",
    "                    (x,y,w,h)=face\n",
    "                    face_img=frame[y:y+h,x:x+w]\n",
    "                    face_img=cv2.resize(face_img,(160,160))\n",
    "                    embed=face_to_embedding(face_img,embed_model)         \n",
    "                    Nembed=Normalizer(norm='l2').transform(embed.reshape(1,-1))\n",
    "                    pred=FR_model.predict_proba(Nembed)\n",
    "                    clas=np.argmax(pred[0])\n",
    "                    prob=pred[0][clas]\n",
    "                    if  prob >.45:\n",
    "                        label=people_dict[clas]+(\" %.2f\"%(prob*100))\n",
    "                    else:\n",
    "                        label='unknown'\n",
    "                    cv2.rectangle(frame, (x,y), (x+w,y+h), (255,255,0), 1)\n",
    "                    cv2.putText(frame,label, (int(x), int(y-15)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0), 2)\n",
    "#                    out.write(frame)\n",
    "        cv2.imshow('Face Recognition',frame)\n",
    "        if cv2.waitKey(1)==ord('q'):\n",
    "            break   \n",
    "finally:            \n",
    "    cap.release()\n",
    "#    out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
