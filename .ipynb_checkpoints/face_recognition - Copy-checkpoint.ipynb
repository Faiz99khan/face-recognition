{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isdir\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def face_extract(filename,required_size=(160,160)):  \n",
    "    image = Image.open(filename)    \n",
    "    image = image.convert('RGB')                # convert to RGB, if needed    \n",
    "    image = np.asarray(image)   \n",
    "    detector = MTCNN()                          # create the detector, using default weights  \n",
    "    results = detector.detect_faces(image)     # detect faces in the image    \n",
    "    if len(results):\n",
    "        faces,boxes=[],[]\n",
    "        for i in range(len(results)):\n",
    "            x1, y1, width, height = results[i]['box']    # extract the bounding box from the first face    \n",
    "            x1, y1 = abs(x1), abs(y1)\n",
    "            x2, y2 = x1 + width, y1 + height\n",
    "            face = image[y1:y2, x1:x2]                  # extract the face\n",
    "            face=cv2.resize(face,required_size)\n",
    "            faces.append(face)\n",
    "            boxes.append([x1,y1,width,height])\n",
    "        return faces,boxes\n",
    "    else:\n",
    "        return ([],[]) \n",
    "    \n",
    "# load images and extract faces for all images in a directory\n",
    "def load_faces(directory):\n",
    "    faces_lst = list()  \n",
    "    for filename in listdir(directory):      \n",
    "        path = directory + filename\n",
    "        faces,_ = face_extract(path)                # get face\n",
    "        if len(faces):\n",
    "            faces_lst.append(faces[0])\n",
    "    return faces_lst\n",
    "\n",
    "\n",
    "# load a dataset that contains one subdir for each class that in turn contains images\n",
    "def load_dataset(directory):\n",
    "    X, y = list(), list()\n",
    "    people_dict={}\n",
    "    for i,subdir in enumerate(listdir(directory)):\n",
    "        path = directory + subdir + '/'        \n",
    "        if not isdir(path):               # skip any files that might be in the dir\n",
    "            continue\n",
    "        faces = load_faces(path)          # load all faces in the subdirectory\n",
    "        people_dict.update({i:subdir})    # create key to people dictionary\n",
    "        print('loaded %d images for person: %s' % (len(faces), subdir))\n",
    "        labels=[subdir for _ in range(len(faces))]\n",
    "        X.extend(faces)\n",
    "        y.extend(labels)\n",
    "    return np.asarray(X), np.asarray(y),people_dict\n",
    "\n",
    "def face_to_embedding(face,model):\n",
    "        face = face.astype('float32')      \n",
    "        mean, std = face.mean(), face.std()\n",
    "        face_norm = (face - mean) / std        # normalize face pixels\n",
    "        face_norm = np.expand_dims(face_norm, axis=0)       \n",
    "        return model.predict(face_norm)        # make prediction to get embedding and return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create new database and train the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 22 images for person: Akshay\n",
      "loaded 26 images for person: Amir\n",
      "loaded 20 images for person: Faiz\n",
      "loaded 13 images for person: Imran\n",
      "loaded 17 images for person: salman\n",
      "loaded 25 images for person: salman khan\n",
      "loaded 13 images for person: shahrukh\n",
      "x_train shape: (136, 160, 160, 3) \n",
      "y_train shape: (136,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is loaded successfully\n",
      "embeddings & people dictionary are saved successfully\n",
      "Accuracy: train=100.000\n",
      "System is trained for face recognition sucessfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load train dataset\n",
    "x_train,y_train,people_dict = load_dataset('my_dataset/train/')\n",
    "print('x_train shape:',x_train.shape,'\\ny_train shape:', y_train.shape)\n",
    "\n",
    "people_to_key_dict={v:k for k,v in people_dict.items()}\n",
    "\n",
    "#load pre-trained facenet model\n",
    "embed_model=load_model('facenet_model/facenet_keras.h5')\n",
    "print('model is loaded successfully')\n",
    "\n",
    "#convert face to embedding\n",
    "x_train_embed=[]\n",
    "y_train_ls=[]\n",
    "for i in range(x_train.shape[0]):\n",
    "    embed=face_to_embedding(x_train[i],embed_model)\n",
    "    x_train_embed.append(embed)\n",
    "    y_train_ls.append(people_to_key_dict[y_train[i]])\n",
    "\n",
    "x_train_embed=np.asarray(x_train_embed).squeeze()\n",
    "y_train=np.asarray(y_train_ls).squeeze()\n",
    "\n",
    "# save embeddings in compressed format\n",
    "np.savez_compressed('database/people-embeddings.npz',x_train_embed, y_train)\n",
    "#save people dictionary \n",
    "joblib.dump(people_dict,'database/people_dictionary.pkl')\n",
    "print('embeddings & people dictionary are saved successfully')\n",
    "\n",
    "#normalize the embeddings\n",
    "l2_normalizer=Normalizer(norm='l2')\n",
    "x_train_embed=l2_normalizer.transform(x_train_embed)\n",
    "\n",
    "#shuffle the data\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(x_train_embed)\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(y_train)\n",
    "\n",
    "#create SVC model and fit \n",
    "FR_model = SVC(kernel='linear', probability=True)\n",
    "FR_model.fit(x_train_embed,y_train)\n",
    "\n",
    "# predict\n",
    "yhat_train = FR_model.predict(x_train_embed)\n",
    "\n",
    "score_train = accuracy_score(y_train, yhat_train)\n",
    "print('Accuracy: train=%.3f' % (score_train*100))\n",
    "\n",
    "#save svc model\n",
    "joblib.dump(FR_model,'model/FR_model.pkl')\n",
    "print('System is trained for face recognition sucessfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add new person in the existing database and train the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c08eb111b9e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m##load the model and people dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mFR_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model/FR_model.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0membed_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'facenet_model/facenet_keras.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpeople_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'database/people_dictionary.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "##load the model and people dictionary\n",
    "FR_model=joblib.load('model/FR_model.pkl')\n",
    "embed_model=load_model('facenet_model/facenet_keras.h5')\n",
    "people_dict=joblib.load('database/people_dictionary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27,) (4,)\n",
      "embeddings & people dictionary are saved successfully\n",
      "Accuracy: train=74.194\n",
      "New person is added in the database sucessfully\n"
     ]
    }
   ],
   "source": [
    "person_name='faizan'\n",
    "directory_path=''\n",
    "\n",
    "def add_new_person(person_name,dir_path):\n",
    "    temp_dict=people_dict.copy()\n",
    "    new_key=len(people_dict)\n",
    "    temp_dict[new_key]=person_name\n",
    "\n",
    "    #load faces\n",
    "    faces=load_faces(dir_path)\n",
    "\n",
    "    new_embed_set=[]\n",
    "    new_label_set=[]\n",
    "    for i in range(len(faces)):\n",
    "        embed=face_to_embedding(faces[i],embed_model)\n",
    "        new_embed_set.append(embed)\n",
    "        new_label_set.append(new_key)\n",
    "\n",
    "    new_embed_set=np.array(new_embed_set).squeeze()\n",
    "    new_label_set=np.array(new_label_set).squeeze()\n",
    "\n",
    "\n",
    "    data=np.load('embedding/tmp_dataset-embeddings.npz')\n",
    "    x_train_embed,y_train=data['arr_0'],data['arr_1']\n",
    "    x_train_embed=np.vstack((x_train_embed,new_embed_set))\n",
    "    y_train=np.concatenate((y_train,new_label_set))\n",
    "    \n",
    "    # save embeddings in compressed format\n",
    "    np.savez_compressed('embedding/tmp_dataset-embeddings.npz',x_train_embed, y_train)\n",
    "    #save people dictionary \n",
    "    joblib.dump(people_dict,'people/tmp_dictionary.pkl')\n",
    "    print('embeddings & people dictionary are saved successfully')\n",
    "\n",
    "    #normalize the embeddings\n",
    "    l2_normalizer=Normalizer(norm='l2')\n",
    "    x_train_embed=l2_normalizer.transform(x_train_embed)\n",
    "\n",
    "    #shuffle the data\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(x_train_embed)\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(y_train)\n",
    "\n",
    "    #create SVC model and fit \n",
    "    FR_model = SVC(kernel='linear', probability=True)\n",
    "    FR_model.fit(x_train_embed,y_train)\n",
    "\n",
    "    # predict\n",
    "    yhat_train = FR_model.predict(x_train_embed)\n",
    "\n",
    "    score_train = accuracy_score(y_train, yhat_train)\n",
    "    print('Accuracy: train=%.3f' % (score_train*100))\n",
    "\n",
    "    #save svc model\n",
    "    joblib.dump(FR_model,'model/tmp_FR_model.pkl')\n",
    "    \n",
    "    print('%s is added in the database sucessfully'%(person_name))\n",
    "    \n",
    "add_new_person(person_name,directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "#load the model and people dictionary\n",
    "FR_model=joblib.load('model/FR_model.pkl')\n",
    "embed_model=load_model('facenet_model/facenet_keras.h5')\n",
    "people_dict=joblib.load('database/people_dictionary.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'images/faiz.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-22da9dddb6a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNembed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwho_is_this\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mFR_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.65\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'This is a %s'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeople_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-22da9dddb6a4>\u001b[0m in \u001b[0;36mwho_is_this\u001b[1;34m(img_path, model, embed_model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'images/faiz.jpg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwho_is_this\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mface\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mface_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0membed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mface_to_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membed_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mNembed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNormalizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-74957a90fd97>\u001b[0m in \u001b[0;36mface_extract\u001b[1;34m(filename, required_size)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mface_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrequired_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m160\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m160\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# convert to RGB, if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2808\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2809\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2810\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images/faiz.jpg'"
     ]
    }
   ],
   "source": [
    "image_path='faiz.jpg'\n",
    "def find_people(img_path,model,embed_model):\n",
    "    faces,boxes=face_extract(img_path)\n",
    "    if len(faces):\n",
    "        img=cv2.imread(image_path,1)\n",
    "        for i in range(len(faces)):\n",
    "            face,(x,y,w,h)=faces[i],boxes[i]\n",
    "            embed=face_to_embedding(face,embed_model)\n",
    "            Nembed=Normalizer(norm='l2').transform(embed)\n",
    "            pred=model.predict_proba(Nembed)\n",
    "            prob=pred[0][np.argmax(pred[0])]\n",
    "            label=people_dict[np.argmax(pred[0])]+(\" %.2f\"%(prob*100)) if prob >.65 else 'unknown'                      \n",
    "            cv2.rectangle(img,(x,y), (x+w,y+h), (255,255,0), 1)\n",
    "            cv2.putText(img,label, (int(x), int(y-15)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,0), 2)\n",
    "        try:\n",
    "\n",
    "            cv2.imshow('image',img)\n",
    "            if cv2.waitKey(0)==ord('q'):\n",
    "                cv2.destroyAllWindows() \n",
    "        finally:\n",
    "            cv2.destroyAllWindows()\n",
    "    else:\n",
    "        print('there is no face in the image')\n",
    "\n",
    "find_people(image_path,FR_model,embed_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### at real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_frontalface_default.xml') #extract face feature\n",
    "cap=cv2.VideoCapture(0)\n",
    "try: \n",
    "    while True:\n",
    "        _,frame=cap.read()\n",
    "        faces = face_cascade.detectMultiScale(frame, 1.3, 5)\n",
    "        if len(faces):\n",
    "            for face in faces:\n",
    "                    (x,y,w,h)=face\n",
    "                    face_img=frame[y:y+h,x:x+w]\n",
    "                    face_img=cv2.resize(face_img,(160,160))\n",
    "                    embed=face_to_embedding(face_img,embed_model)         \n",
    "                    Nembed=Normalizer(norm='l2').transform(embed.reshape(1,-1))\n",
    "                    pred=FR_model.predict_proba(Nembed)\n",
    "                    clas=np.argmax(pred[0])\n",
    "                    prob=pred[0][clas]\n",
    "                    if  prob >.45:\n",
    "                        label=people_dict[clas]+(\" %.2f\"%(prob*100))\n",
    "                    else:\n",
    "                        label='unknown'\n",
    "                    cv2.rectangle(frame, (x,y), (x+w,y+h), (255,255,0), 1)\n",
    "                    cv2.putText(frame,label, (int(x), int(y-15)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0), 2)\n",
    "        cv2.imshow('Face Recognition',frame)\n",
    "        if cv2.waitKey(1)==ord('q'):\n",
    "            break   \n",
    "finally:            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
